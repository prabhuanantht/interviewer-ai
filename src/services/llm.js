// Basic abstraction for LLM calls. 
// Currently tailored for Gemini (Google Generative AI) as it's often free/easier to access, 
// but structured to allow swapping.

const SYSTEM_PROMPT_TEMPLATE = (role, difficulty) => `
You are an expert interviewer conducting a mock interview for a ${role} position.
The difficulty level is ${difficulty}.

Your goal is to conduct a realistic, professional interview.
1.  **Questioning**: Ask one relevant question at a time. Start with an introduction.
2.  **Adaptability**:
    *   If the user is **confused** or asks for help, provide a hint or rephrase the question, then gently nudge them back to the answer.
    *   If the user is **efficient** (short answers), ask deeper follow-up questions to probe their understanding.
    *   If the user is **chatty** or goes off-topic, politely acknowledge their point but steer the conversation back to the interview topic immediately.
3.  **Tone**: Professional, encouraging, but objective.
4.  **Format**: Keep your responses concise (under 3-4 sentences) to facilitate a natural voice conversation. Do NOT write long paragraphs.
5.  **Feedback**: Do NOT provide a full evaluation during the interview. Only provide brief neutral acknowledgments (e.g., "I see," "Interesting point") before moving to the next question.

Start by introducing yourself as the AI interviewer and asking the first question (e.g., "Tell me about yourself").
`;

export async function generateResponse(apiKey, messages, role, difficulty) {
    if (!apiKey) throw new Error("API Key is missing");

    // This is a placeholder for the actual API call.
    // We will likely use the fetch API to call the provider's endpoint directly
    // to avoid needing a backend proxy for this demo.

    // For Gemini:
    const API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;

    const systemInstruction = SYSTEM_PROMPT_TEMPLATE(role, difficulty);

    // Format messages for Gemini
    // Gemini expects: { role: "user" | "model", parts: [{ text: "..." }] }
    // We need to prepend the system prompt or include it in the first message if system instruction isn't supported in the specific endpoint version used purely via REST without the SDK.
    // 1.5 Flash supports system_instruction.

    const contents = messages.map(m => ({
        role: m.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: m.content }]
    }));

    try {
        const response = await fetch(API_URL, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                system_instruction: {
                    parts: [{ text: systemInstruction }]
                },
                contents: contents,
                generationConfig: {
                    maxOutputTokens: 500, // Increased to prevent MAX_TOKENS error
                    temperature: 0.7,
                }
            })
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error?.message || 'Failed to fetch response');
        }

        const data = await response.json();

        if (!data.candidates || data.candidates.length === 0) {
            console.error("No candidates returned:", data);
            if (data.promptFeedback) {
                throw new Error(`Safety block: ${data.promptFeedback.blockReason}`);
            }
            throw new Error("No response generated by the model.");
        }

        const candidate = data.candidates[0];
        if (!candidate.content || !candidate.content.parts || !candidate.content.parts[0]) {
            console.error("Invalid candidate structure:", candidate);
            throw new Error("Model returned an incomplete response.");
        }

        return candidate.content.parts[0].text;

    } catch (error) {
        console.error("LLM Error:", error);
        throw error;
    }
}

export async function generateFeedback(apiKey, messages, role) {
    // Similar to above, but asks for a JSON report card.
    const API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;

    const prompt = `
  The interview is over. Here is the transcript:
  ${JSON.stringify(messages)}
  
  Please evaluate the candidate for the ${role} role.
  Return a JSON object with the following structure:
  {
    "score": number (0-100),
    "strengths": ["string", "string"],
    "weaknesses": ["string", "string"],
    "summary": "string"
  }
  Do not include markdown formatting like \`\`\`json. Just the raw JSON.
  `;

    try {
        const response = await fetch(API_URL, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                contents: [{ role: 'user', parts: [{ text: prompt }] }]
            })
        });

        const data = await response.json();

        if (!data.candidates || data.candidates.length === 0) {
            throw new Error("No feedback generated.");
        }

        const candidate = data.candidates[0];
        if (!candidate.content || !candidate.content.parts || !candidate.content.parts[0]) {
            throw new Error("Invalid feedback structure.");
        }

        const text = candidate.content.parts[0].text;
        // Clean up potential markdown code blocks if the model ignores the instruction
        const cleanText = text.replace(/```json/g, '').replace(/```/g, '').trim();
        return JSON.parse(cleanText);
    } catch (error) {
        console.error("Feedback Error:", error);
        return { score: 0, strengths: [], weaknesses: ["Error generating feedback"], summary: "Could not generate report." };
    }
}
